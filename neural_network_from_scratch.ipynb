{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights= 0.10 * np.random.randn(n_inputs,n_neurons)  #multiple with 0.1 to get vslues in 0. somethiing\n",
    "        self.bias= np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = np.dot(input, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLu:\n",
    "    def forward(self,input):\n",
    "        self.output=np.maximum(0,input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self,input):\n",
    "        exp_value=np.exp(input-np.max(input, axis=1,keepdims=True))\n",
    "        probability=exp_value/np.sum(exp_value,axis=1,keepdims=True)\n",
    "        self.output=probability\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses=self.forward(output,y)\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples=len(y_pred)\n",
    "        y_pred_clipped=np.clip(y_pred, 1e-7,1-1e-7)\n",
    "\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidence=y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidence=np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        negative_log_liklihoods= -np.log(correct_confidence)\n",
    "        return negative_log_liklihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=vertical_data(samples=100,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1=Layer_Dense(2,3)\n",
    "activation1=Activation_ReLu()\n",
    "dense2=Layer_Dense(3,3)\n",
    "activation2=Activation_Softmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=Loss_CategoricalCrossEntropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_loss=9999999\n",
    "best_dense1_weights=dense1.weights.copy()\n",
    "best_dense1_bias=dense1.bias.copy()\n",
    "best_dense2_weights=dense2.weights.copy()\n",
    "best_dense2_bias=dense2.bias.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  loss:  1.0984223  accuracy:  0.3333333333333333\n",
      "iteration:  4  loss:  1.0983797  accuracy:  0.3333333333333333\n",
      "iteration:  5  loss:  1.0973538  accuracy:  0.33666666666666667\n",
      "iteration:  6  loss:  1.0963587  accuracy:  0.26666666666666666\n",
      "iteration:  8  loss:  1.0953789  accuracy:  0.34\n",
      "iteration:  14  loss:  1.095357  accuracy:  0.3333333333333333\n",
      "iteration:  21  loss:  1.0940696  accuracy:  0.3333333333333333\n",
      "iteration:  24  loss:  1.0937893  accuracy:  0.3333333333333333\n",
      "iteration:  25  loss:  1.0903312  accuracy:  0.62\n",
      "iteration:  27  loss:  1.0894265  accuracy:  0.3333333333333333\n",
      "iteration:  28  loss:  1.0862789  accuracy:  0.3333333333333333\n",
      "iteration:  29  loss:  1.0799779  accuracy:  0.5933333333333334\n",
      "iteration:  30  loss:  1.0792155  accuracy:  0.48\n",
      "iteration:  31  loss:  1.0745  accuracy:  0.4666666666666667\n",
      "iteration:  32  loss:  1.0684829  accuracy:  0.66\n",
      "iteration:  38  loss:  1.0629193  accuracy:  0.45\n",
      "iteration:  39  loss:  1.0615517  accuracy:  0.43\n",
      "iteration:  40  loss:  1.0595874  accuracy:  0.4266666666666667\n",
      "iteration:  44  loss:  1.0584913  accuracy:  0.5966666666666667\n",
      "iteration:  47  loss:  1.0482612  accuracy:  0.65\n",
      "iteration:  49  loss:  1.0481751  accuracy:  0.6533333333333333\n",
      "iteration:  51  loss:  1.0454738  accuracy:  0.6666666666666666\n",
      "iteration:  52  loss:  1.0411766  accuracy:  0.6666666666666666\n",
      "iteration:  58  loss:  1.0346273  accuracy:  0.6266666666666667\n",
      "iteration:  60  loss:  1.0305107  accuracy:  0.6533333333333333\n",
      "iteration:  62  loss:  1.026624  accuracy:  0.6666666666666666\n",
      "iteration:  63  loss:  1.0231382  accuracy:  0.6633333333333333\n",
      "iteration:  65  loss:  1.0115566  accuracy:  0.66\n",
      "iteration:  69  loss:  1.0097806  accuracy:  0.65\n",
      "iteration:  70  loss:  0.99939394  accuracy:  0.6633333333333333\n",
      "iteration:  72  loss:  0.9957546  accuracy:  0.6566666666666666\n",
      "iteration:  73  loss:  0.9936532  accuracy:  0.6566666666666666\n",
      "iteration:  74  loss:  0.99186826  accuracy:  0.6666666666666666\n",
      "iteration:  75  loss:  0.9857624  accuracy:  0.6666666666666666\n",
      "iteration:  77  loss:  0.97249204  accuracy:  0.6666666666666666\n",
      "iteration:  78  loss:  0.9715173  accuracy:  0.6666666666666666\n",
      "iteration:  82  loss:  0.96550107  accuracy:  0.6666666666666666\n",
      "iteration:  86  loss:  0.9562752  accuracy:  0.6633333333333333\n",
      "iteration:  87  loss:  0.94829357  accuracy:  0.66\n",
      "iteration:  88  loss:  0.9442397  accuracy:  0.65\n",
      "iteration:  90  loss:  0.94142336  accuracy:  0.6666666666666666\n",
      "iteration:  92  loss:  0.93910563  accuracy:  0.6633333333333333\n",
      "iteration:  93  loss:  0.93255043  accuracy:  0.6666666666666666\n",
      "iteration:  96  loss:  0.93097395  accuracy:  0.6633333333333333\n",
      "iteration:  102  loss:  0.92009765  accuracy:  0.6666666666666666\n",
      "iteration:  106  loss:  0.9145204  accuracy:  0.6533333333333333\n",
      "iteration:  107  loss:  0.91186756  accuracy:  0.6633333333333333\n",
      "iteration:  109  loss:  0.90856874  accuracy:  0.65\n",
      "iteration:  110  loss:  0.9047895  accuracy:  0.6633333333333333\n",
      "iteration:  112  loss:  0.9024719  accuracy:  0.6533333333333333\n",
      "iteration:  113  loss:  0.9015204  accuracy:  0.66\n",
      "iteration:  114  loss:  0.88472  accuracy:  0.6666666666666666\n",
      "iteration:  116  loss:  0.88188744  accuracy:  0.6666666666666666\n",
      "iteration:  117  loss:  0.8599644  accuracy:  0.6666666666666666\n",
      "iteration:  119  loss:  0.8565385  accuracy:  0.6666666666666666\n",
      "iteration:  120  loss:  0.84650195  accuracy:  0.6666666666666666\n",
      "iteration:  123  loss:  0.84608734  accuracy:  0.6666666666666666\n",
      "iteration:  125  loss:  0.8450085  accuracy:  0.66\n",
      "iteration:  127  loss:  0.8391611  accuracy:  0.6633333333333333\n",
      "iteration:  128  loss:  0.83855325  accuracy:  0.6633333333333333\n",
      "iteration:  135  loss:  0.8300954  accuracy:  0.7366666666666667\n",
      "iteration:  142  loss:  0.82183176  accuracy:  0.7333333333333333\n",
      "iteration:  143  loss:  0.8134249  accuracy:  0.6833333333333333\n",
      "iteration:  145  loss:  0.804559  accuracy:  0.6633333333333333\n",
      "iteration:  146  loss:  0.80114627  accuracy:  0.6633333333333333\n",
      "iteration:  151  loss:  0.795582  accuracy:  0.6566666666666666\n",
      "iteration:  152  loss:  0.7831929  accuracy:  0.7066666666666667\n",
      "iteration:  153  loss:  0.7803186  accuracy:  0.8066666666666666\n",
      "iteration:  160  loss:  0.7709501  accuracy:  0.7133333333333334\n",
      "iteration:  162  loss:  0.75929654  accuracy:  0.7666666666666667\n",
      "iteration:  170  loss:  0.75891995  accuracy:  0.8466666666666667\n",
      "iteration:  173  loss:  0.7436351  accuracy:  0.89\n",
      "iteration:  174  loss:  0.7397509  accuracy:  0.91\n",
      "iteration:  177  loss:  0.7290546  accuracy:  0.9166666666666666\n",
      "iteration:  178  loss:  0.7221644  accuracy:  0.88\n",
      "iteration:  179  loss:  0.71207976  accuracy:  0.8233333333333334\n",
      "iteration:  186  loss:  0.70118314  accuracy:  0.84\n",
      "iteration:  187  loss:  0.69867086  accuracy:  0.8766666666666667\n",
      "iteration:  191  loss:  0.69401044  accuracy:  0.9166666666666666\n",
      "iteration:  192  loss:  0.68178225  accuracy:  0.8966666666666666\n",
      "iteration:  194  loss:  0.6800145  accuracy:  0.8733333333333333\n",
      "iteration:  195  loss:  0.6671578  accuracy:  0.9233333333333333\n",
      "iteration:  197  loss:  0.66547287  accuracy:  0.91\n",
      "iteration:  199  loss:  0.6638206  accuracy:  0.8933333333333333\n",
      "iteration:  207  loss:  0.656307  accuracy:  0.8966666666666666\n",
      "iteration:  209  loss:  0.6466993  accuracy:  0.8533333333333334\n",
      "iteration:  212  loss:  0.64381033  accuracy:  0.9066666666666666\n",
      "iteration:  213  loss:  0.6403273  accuracy:  0.9166666666666666\n",
      "iteration:  216  loss:  0.6377454  accuracy:  0.9066666666666666\n",
      "iteration:  217  loss:  0.62995374  accuracy:  0.91\n",
      "iteration:  218  loss:  0.6299277  accuracy:  0.9133333333333333\n",
      "iteration:  225  loss:  0.6264762  accuracy:  0.93\n",
      "iteration:  229  loss:  0.6185889  accuracy:  0.9133333333333333\n",
      "iteration:  231  loss:  0.6068291  accuracy:  0.9166666666666666\n",
      "iteration:  233  loss:  0.5968861  accuracy:  0.88\n",
      "iteration:  237  loss:  0.59581876  accuracy:  0.9033333333333333\n",
      "iteration:  243  loss:  0.58332413  accuracy:  0.88\n",
      "iteration:  246  loss:  0.5773749  accuracy:  0.88\n",
      "iteration:  247  loss:  0.5746483  accuracy:  0.87\n",
      "iteration:  261  loss:  0.5723176  accuracy:  0.8966666666666666\n",
      "iteration:  266  loss:  0.5657035  accuracy:  0.9033333333333333\n",
      "iteration:  267  loss:  0.5612256  accuracy:  0.9066666666666666\n",
      "iteration:  269  loss:  0.5597009  accuracy:  0.9166666666666666\n",
      "iteration:  271  loss:  0.5582792  accuracy:  0.91\n",
      "iteration:  274  loss:  0.5470919  accuracy:  0.9066666666666666\n",
      "iteration:  277  loss:  0.5444199  accuracy:  0.8833333333333333\n",
      "iteration:  278  loss:  0.5423663  accuracy:  0.8966666666666666\n",
      "iteration:  279  loss:  0.53591704  accuracy:  0.91\n",
      "iteration:  280  loss:  0.5356531  accuracy:  0.9166666666666666\n",
      "iteration:  287  loss:  0.52388126  accuracy:  0.88\n",
      "iteration:  291  loss:  0.5230593  accuracy:  0.9166666666666666\n",
      "iteration:  292  loss:  0.51510304  accuracy:  0.91\n",
      "iteration:  295  loss:  0.5079644  accuracy:  0.9166666666666666\n",
      "iteration:  296  loss:  0.50646853  accuracy:  0.88\n",
      "iteration:  297  loss:  0.505777  accuracy:  0.8766666666666667\n",
      "iteration:  298  loss:  0.50435334  accuracy:  0.8466666666666667\n",
      "iteration:  302  loss:  0.49984887  accuracy:  0.88\n",
      "iteration:  304  loss:  0.49160206  accuracy:  0.89\n",
      "iteration:  308  loss:  0.4849913  accuracy:  0.8866666666666667\n",
      "iteration:  312  loss:  0.48491332  accuracy:  0.87\n",
      "iteration:  313  loss:  0.4825356  accuracy:  0.8733333333333333\n",
      "iteration:  317  loss:  0.47062474  accuracy:  0.91\n",
      "iteration:  318  loss:  0.47040364  accuracy:  0.91\n",
      "iteration:  319  loss:  0.46937576  accuracy:  0.88\n",
      "iteration:  321  loss:  0.4668751  accuracy:  0.8766666666666667\n",
      "iteration:  322  loss:  0.4659624  accuracy:  0.8866666666666667\n",
      "iteration:  323  loss:  0.46302938  accuracy:  0.9\n",
      "iteration:  324  loss:  0.46126094  accuracy:  0.8933333333333333\n",
      "iteration:  327  loss:  0.44551045  accuracy:  0.9033333333333333\n",
      "iteration:  328  loss:  0.444788  accuracy:  0.9066666666666666\n",
      "iteration:  329  loss:  0.44041774  accuracy:  0.9166666666666666\n",
      "iteration:  339  loss:  0.43971008  accuracy:  0.92\n",
      "iteration:  343  loss:  0.43664545  accuracy:  0.9166666666666666\n",
      "iteration:  344  loss:  0.43264312  accuracy:  0.9066666666666666\n",
      "iteration:  345  loss:  0.43105114  accuracy:  0.91\n",
      "iteration:  351  loss:  0.42785248  accuracy:  0.9133333333333333\n",
      "iteration:  355  loss:  0.4272226  accuracy:  0.9133333333333333\n",
      "iteration:  359  loss:  0.4268541  accuracy:  0.91\n",
      "iteration:  360  loss:  0.41957077  accuracy:  0.9166666666666666\n",
      "iteration:  366  loss:  0.4158568  accuracy:  0.9166666666666666\n",
      "iteration:  380  loss:  0.41116476  accuracy:  0.9133333333333333\n",
      "iteration:  383  loss:  0.4090681  accuracy:  0.9033333333333333\n",
      "iteration:  385  loss:  0.40334335  accuracy:  0.9166666666666666\n",
      "iteration:  386  loss:  0.39778107  accuracy:  0.9133333333333333\n",
      "iteration:  388  loss:  0.39515197  accuracy:  0.9266666666666666\n",
      "iteration:  394  loss:  0.3826742  accuracy:  0.9133333333333333\n",
      "iteration:  396  loss:  0.38090768  accuracy:  0.91\n",
      "iteration:  398  loss:  0.37871838  accuracy:  0.9133333333333333\n",
      "iteration:  399  loss:  0.3760018  accuracy:  0.9166666666666666\n",
      "iteration:  407  loss:  0.3754516  accuracy:  0.91\n",
      "iteration:  408  loss:  0.3713687  accuracy:  0.92\n",
      "iteration:  422  loss:  0.36642024  accuracy:  0.9133333333333333\n",
      "iteration:  423  loss:  0.36478946  accuracy:  0.9166666666666666\n",
      "iteration:  425  loss:  0.36034265  accuracy:  0.9166666666666666\n",
      "iteration:  433  loss:  0.35883766  accuracy:  0.9066666666666666\n",
      "iteration:  450  loss:  0.3575972  accuracy:  0.9133333333333333\n",
      "iteration:  459  loss:  0.35390022  accuracy:  0.9133333333333333\n",
      "iteration:  460  loss:  0.35381114  accuracy:  0.91\n",
      "iteration:  471  loss:  0.35312453  accuracy:  0.93\n",
      "iteration:  475  loss:  0.3452771  accuracy:  0.9266666666666666\n",
      "iteration:  477  loss:  0.3396921  accuracy:  0.9133333333333333\n",
      "iteration:  488  loss:  0.33730936  accuracy:  0.9166666666666666\n",
      "iteration:  489  loss:  0.33459455  accuracy:  0.92\n",
      "iteration:  493  loss:  0.3333933  accuracy:  0.91\n",
      "iteration:  502  loss:  0.3312764  accuracy:  0.9133333333333333\n",
      "iteration:  513  loss:  0.32936457  accuracy:  0.9233333333333333\n",
      "iteration:  516  loss:  0.32686925  accuracy:  0.9166666666666666\n",
      "iteration:  518  loss:  0.32224885  accuracy:  0.9133333333333333\n",
      "iteration:  521  loss:  0.31795046  accuracy:  0.9166666666666666\n",
      "iteration:  541  loss:  0.3162995  accuracy:  0.9233333333333333\n",
      "iteration:  542  loss:  0.31360736  accuracy:  0.9133333333333333\n",
      "iteration:  545  loss:  0.313054  accuracy:  0.91\n",
      "iteration:  548  loss:  0.30562922  accuracy:  0.91\n",
      "iteration:  549  loss:  0.30486923  accuracy:  0.9133333333333333\n",
      "iteration:  550  loss:  0.30165523  accuracy:  0.91\n",
      "iteration:  551  loss:  0.2972047  accuracy:  0.9166666666666666\n",
      "iteration:  553  loss:  0.29526633  accuracy:  0.9166666666666666\n",
      "iteration:  563  loss:  0.29265034  accuracy:  0.9166666666666666\n",
      "iteration:  568  loss:  0.291126  accuracy:  0.9066666666666666\n",
      "iteration:  570  loss:  0.28904536  accuracy:  0.91\n",
      "iteration:  577  loss:  0.28651813  accuracy:  0.9166666666666666\n",
      "iteration:  580  loss:  0.28548133  accuracy:  0.92\n",
      "iteration:  587  loss:  0.28527153  accuracy:  0.92\n",
      "iteration:  588  loss:  0.28249323  accuracy:  0.9133333333333333\n",
      "iteration:  599  loss:  0.28165466  accuracy:  0.9233333333333333\n",
      "iteration:  604  loss:  0.2804828  accuracy:  0.9133333333333333\n",
      "iteration:  612  loss:  0.2775934  accuracy:  0.9166666666666666\n",
      "iteration:  617  loss:  0.27684566  accuracy:  0.91\n",
      "iteration:  619  loss:  0.2748175  accuracy:  0.91\n",
      "iteration:  620  loss:  0.27415913  accuracy:  0.9166666666666666\n",
      "iteration:  623  loss:  0.2722077  accuracy:  0.9166666666666666\n",
      "iteration:  628  loss:  0.27128276  accuracy:  0.92\n",
      "iteration:  632  loss:  0.26841226  accuracy:  0.91\n",
      "iteration:  634  loss:  0.26680222  accuracy:  0.9166666666666666\n",
      "iteration:  644  loss:  0.2630745  accuracy:  0.9133333333333333\n",
      "iteration:  677  loss:  0.26073983  accuracy:  0.9166666666666666\n",
      "iteration:  693  loss:  0.25914758  accuracy:  0.92\n",
      "iteration:  697  loss:  0.25709885  accuracy:  0.92\n",
      "iteration:  701  loss:  0.2560239  accuracy:  0.92\n",
      "iteration:  703  loss:  0.2551435  accuracy:  0.9133333333333333\n",
      "iteration:  704  loss:  0.24862146  accuracy:  0.9133333333333333\n",
      "iteration:  730  loss:  0.24789062  accuracy:  0.9166666666666666\n",
      "iteration:  732  loss:  0.24615943  accuracy:  0.9166666666666666\n",
      "iteration:  736  loss:  0.24508351  accuracy:  0.9133333333333333\n",
      "iteration:  769  loss:  0.24431884  accuracy:  0.9133333333333333\n",
      "iteration:  770  loss:  0.2440983  accuracy:  0.92\n",
      "iteration:  772  loss:  0.24259977  accuracy:  0.9233333333333333\n",
      "iteration:  776  loss:  0.24245366  accuracy:  0.92\n",
      "iteration:  779  loss:  0.24124527  accuracy:  0.92\n",
      "iteration:  794  loss:  0.23603871  accuracy:  0.92\n",
      "iteration:  803  loss:  0.23484334  accuracy:  0.9133333333333333\n",
      "iteration:  810  loss:  0.2335619  accuracy:  0.92\n",
      "iteration:  813  loss:  0.23278211  accuracy:  0.9166666666666666\n",
      "iteration:  815  loss:  0.23249803  accuracy:  0.92\n",
      "iteration:  817  loss:  0.2299141  accuracy:  0.9133333333333333\n",
      "iteration:  822  loss:  0.22932474  accuracy:  0.9233333333333333\n",
      "iteration:  826  loss:  0.22873057  accuracy:  0.92\n",
      "iteration:  835  loss:  0.22621982  accuracy:  0.9166666666666666\n",
      "iteration:  850  loss:  0.22541356  accuracy:  0.92\n",
      "iteration:  853  loss:  0.22368245  accuracy:  0.9166666666666666\n",
      "iteration:  856  loss:  0.22317614  accuracy:  0.9166666666666666\n",
      "iteration:  857  loss:  0.22287779  accuracy:  0.9133333333333333\n",
      "iteration:  863  loss:  0.22190864  accuracy:  0.9166666666666666\n",
      "iteration:  870  loss:  0.22000691  accuracy:  0.9266666666666666\n",
      "iteration:  876  loss:  0.2188155  accuracy:  0.9133333333333333\n",
      "iteration:  884  loss:  0.21859194  accuracy:  0.9233333333333333\n",
      "iteration:  885  loss:  0.2180812  accuracy:  0.9266666666666666\n",
      "iteration:  893  loss:  0.21574478  accuracy:  0.9266666666666666\n",
      "iteration:  898  loss:  0.21436536  accuracy:  0.9233333333333333\n",
      "iteration:  899  loss:  0.21346405  accuracy:  0.92\n",
      "iteration:  904  loss:  0.2130826  accuracy:  0.9266666666666666\n",
      "iteration:  909  loss:  0.2115339  accuracy:  0.9266666666666666\n",
      "iteration:  921  loss:  0.21120135  accuracy:  0.9266666666666666\n",
      "iteration:  944  loss:  0.21057583  accuracy:  0.9166666666666666\n",
      "iteration:  945  loss:  0.20920143  accuracy:  0.9266666666666666\n",
      "iteration:  946  loss:  0.20723419  accuracy:  0.9166666666666666\n",
      "iteration:  950  loss:  0.20606679  accuracy:  0.9166666666666666\n",
      "iteration:  970  loss:  0.20603906  accuracy:  0.9266666666666666\n",
      "iteration:  975  loss:  0.20593467  accuracy:  0.9266666666666666\n",
      "iteration:  978  loss:  0.20513871  accuracy:  0.9266666666666666\n",
      "iteration:  985  loss:  0.20460014  accuracy:  0.92\n",
      "iteration:  986  loss:  0.20434561  accuracy:  0.92\n",
      "iteration:  991  loss:  0.20289434  accuracy:  0.9266666666666666\n",
      "iteration:  992  loss:  0.2027089  accuracy:  0.93\n",
      "iteration:  996  loss:  0.20241767  accuracy:  0.92\n",
      "iteration:  1001  loss:  0.20226936  accuracy:  0.93\n",
      "iteration:  1002  loss:  0.20213412  accuracy:  0.93\n",
      "iteration:  1004  loss:  0.20153312  accuracy:  0.9266666666666666\n",
      "iteration:  1009  loss:  0.20090997  accuracy:  0.9266666666666666\n",
      "iteration:  1010  loss:  0.20010999  accuracy:  0.92\n",
      "iteration:  1013  loss:  0.19837359  accuracy:  0.92\n",
      "iteration:  1017  loss:  0.19816655  accuracy:  0.9266666666666666\n",
      "iteration:  1024  loss:  0.19739637  accuracy:  0.93\n",
      "iteration:  1031  loss:  0.19728427  accuracy:  0.9333333333333333\n",
      "iteration:  1032  loss:  0.19691966  accuracy:  0.9333333333333333\n",
      "iteration:  1035  loss:  0.19583611  accuracy:  0.9233333333333333\n",
      "iteration:  1036  loss:  0.19569984  accuracy:  0.9266666666666666\n",
      "iteration:  1047  loss:  0.19433995  accuracy:  0.9266666666666666\n",
      "iteration:  1063  loss:  0.19433871  accuracy:  0.93\n",
      "iteration:  1070  loss:  0.19310279  accuracy:  0.9266666666666666\n",
      "iteration:  1077  loss:  0.193046  accuracy:  0.9266666666666666\n",
      "iteration:  1078  loss:  0.19148956  accuracy:  0.9266666666666666\n",
      "iteration:  1086  loss:  0.1911535  accuracy:  0.9266666666666666\n",
      "iteration:  1090  loss:  0.1905721  accuracy:  0.93\n",
      "iteration:  1091  loss:  0.19053984  accuracy:  0.9333333333333333\n",
      "iteration:  1102  loss:  0.18996018  accuracy:  0.9166666666666666\n",
      "iteration:  1108  loss:  0.1897209  accuracy:  0.92\n",
      "iteration:  1111  loss:  0.18907915  accuracy:  0.9233333333333333\n",
      "iteration:  1127  loss:  0.18856658  accuracy:  0.9266666666666666\n",
      "iteration:  1149  loss:  0.18786791  accuracy:  0.9233333333333333\n",
      "iteration:  1171  loss:  0.1874658  accuracy:  0.9266666666666666\n",
      "iteration:  1233  loss:  0.18745258  accuracy:  0.9266666666666666\n",
      "iteration:  1237  loss:  0.18739212  accuracy:  0.9233333333333333\n",
      "iteration:  1296  loss:  0.18707071  accuracy:  0.9266666666666666\n",
      "iteration:  1355  loss:  0.18653063  accuracy:  0.9266666666666666\n",
      "iteration:  1356  loss:  0.18622386  accuracy:  0.93\n",
      "iteration:  1359  loss:  0.18557604  accuracy:  0.9166666666666666\n",
      "iteration:  1411  loss:  0.18476795  accuracy:  0.92\n",
      "iteration:  1417  loss:  0.18470968  accuracy:  0.93\n",
      "iteration:  1447  loss:  0.18467274  accuracy:  0.9233333333333333\n",
      "iteration:  1453  loss:  0.18455735  accuracy:  0.9133333333333333\n",
      "iteration:  1458  loss:  0.18423183  accuracy:  0.93\n",
      "iteration:  1463  loss:  0.1840565  accuracy:  0.93\n",
      "iteration:  1494  loss:  0.18387927  accuracy:  0.93\n",
      "iteration:  1516  loss:  0.18351577  accuracy:  0.9266666666666666\n",
      "iteration:  1523  loss:  0.18293194  accuracy:  0.93\n",
      "iteration:  1536  loss:  0.18262284  accuracy:  0.9266666666666666\n",
      "iteration:  1539  loss:  0.18231873  accuracy:  0.93\n",
      "iteration:  1544  loss:  0.18229717  accuracy:  0.9266666666666666\n",
      "iteration:  1545  loss:  0.18210493  accuracy:  0.93\n",
      "iteration:  1573  loss:  0.18206626  accuracy:  0.93\n",
      "iteration:  1597  loss:  0.18174757  accuracy:  0.93\n",
      "iteration:  1598  loss:  0.18079473  accuracy:  0.9266666666666666\n",
      "iteration:  1607  loss:  0.18075897  accuracy:  0.9266666666666666\n",
      "iteration:  1673  loss:  0.18057975  accuracy:  0.9266666666666666\n",
      "iteration:  1700  loss:  0.18038839  accuracy:  0.9266666666666666\n",
      "iteration:  1834  loss:  0.18035775  accuracy:  0.93\n",
      "iteration:  1853  loss:  0.18016413  accuracy:  0.9266666666666666\n",
      "iteration:  1871  loss:  0.18005835  accuracy:  0.93\n",
      "iteration:  1877  loss:  0.18000521  accuracy:  0.9266666666666666\n",
      "iteration:  1884  loss:  0.17996305  accuracy:  0.9233333333333333\n",
      "iteration:  1887  loss:  0.1798863  accuracy:  0.9233333333333333\n",
      "iteration:  1889  loss:  0.17965984  accuracy:  0.9266666666666666\n",
      "iteration:  1893  loss:  0.17932247  accuracy:  0.9266666666666666\n",
      "iteration:  1899  loss:  0.17900297  accuracy:  0.93\n",
      "iteration:  1901  loss:  0.17874512  accuracy:  0.93\n",
      "iteration:  1908  loss:  0.1784795  accuracy:  0.9333333333333333\n",
      "iteration:  2013  loss:  0.17813486  accuracy:  0.93\n",
      "iteration:  2052  loss:  0.17798452  accuracy:  0.9333333333333333\n",
      "iteration:  2083  loss:  0.17745107  accuracy:  0.93\n",
      "iteration:  2218  loss:  0.17738856  accuracy:  0.9333333333333333\n",
      "iteration:  2285  loss:  0.17736058  accuracy:  0.93\n",
      "iteration:  2301  loss:  0.17708863  accuracy:  0.93\n",
      "iteration:  2402  loss:  0.17682144  accuracy:  0.9366666666666666\n",
      "iteration:  2462  loss:  0.17647892  accuracy:  0.93\n",
      "iteration:  2490  loss:  0.17641337  accuracy:  0.9333333333333333\n",
      "iteration:  2526  loss:  0.1763622  accuracy:  0.93\n",
      "iteration:  2569  loss:  0.17606516  accuracy:  0.93\n",
      "iteration:  2583  loss:  0.17605008  accuracy:  0.9333333333333333\n",
      "iteration:  2590  loss:  0.17598702  accuracy:  0.9333333333333333\n",
      "iteration:  2768  loss:  0.17582634  accuracy:  0.93\n",
      "iteration:  2808  loss:  0.17579964  accuracy:  0.9333333333333333\n",
      "iteration:  2810  loss:  0.17579381  accuracy:  0.9333333333333333\n",
      "iteration:  2827  loss:  0.17558044  accuracy:  0.9333333333333333\n",
      "iteration:  2900  loss:  0.1755803  accuracy:  0.9333333333333333\n",
      "iteration:  2930  loss:  0.17548019  accuracy:  0.93\n",
      "iteration:  2948  loss:  0.17528725  accuracy:  0.93\n",
      "iteration:  2958  loss:  0.17504217  accuracy:  0.9333333333333333\n",
      "iteration:  3013  loss:  0.17499498  accuracy:  0.9333333333333333\n",
      "iteration:  3035  loss:  0.17494702  accuracy:  0.9333333333333333\n",
      "iteration:  3192  loss:  0.17479554  accuracy:  0.9333333333333333\n",
      "iteration:  3216  loss:  0.17468835  accuracy:  0.9266666666666666\n",
      "iteration:  3226  loss:  0.17468123  accuracy:  0.93\n",
      "iteration:  3237  loss:  0.1746267  accuracy:  0.93\n",
      "iteration:  3245  loss:  0.17455019  accuracy:  0.9266666666666666\n",
      "iteration:  3296  loss:  0.1745092  accuracy:  0.93\n",
      "iteration:  3398  loss:  0.17445773  accuracy:  0.9266666666666666\n",
      "iteration:  3399  loss:  0.17437612  accuracy:  0.93\n",
      "iteration:  3476  loss:  0.17411134  accuracy:  0.93\n",
      "iteration:  3479  loss:  0.17379463  accuracy:  0.9266666666666666\n",
      "iteration:  5277  loss:  0.17379257  accuracy:  0.9266666666666666\n",
      "iteration:  5365  loss:  0.17373194  accuracy:  0.9266666666666666\n",
      "iteration:  5377  loss:  0.17372082  accuracy:  0.9266666666666666\n",
      "iteration:  5416  loss:  0.17370279  accuracy:  0.93\n",
      "iteration:  5459  loss:  0.17367993  accuracy:  0.9266666666666666\n",
      "iteration:  5527  loss:  0.17355452  accuracy:  0.9266666666666666\n",
      "iteration:  5570  loss:  0.17333786  accuracy:  0.93\n",
      "iteration:  5701  loss:  0.17329127  accuracy:  0.93\n",
      "iteration:  5817  loss:  0.17329109  accuracy:  0.93\n",
      "iteration:  5864  loss:  0.17325208  accuracy:  0.9266666666666666\n",
      "iteration:  6347  loss:  0.17320995  accuracy:  0.93\n",
      "iteration:  6603  loss:  0.17320493  accuracy:  0.9266666666666666\n",
      "iteration:  6729  loss:  0.17310017  accuracy:  0.9266666666666666\n",
      "iteration:  7045  loss:  0.17304504  accuracy:  0.93\n",
      "iteration:  7387  loss:  0.17303562  accuracy:  0.9266666666666666\n",
      "iteration:  7606  loss:  0.17296612  accuracy:  0.9266666666666666\n",
      "iteration:  7882  loss:  0.17287931  accuracy:  0.93\n",
      "iteration:  10488  loss:  0.17285775  accuracy:  0.9266666666666666\n",
      "iteration:  11046  loss:  0.17285128  accuracy:  0.93\n",
      "iteration:  11091  loss:  0.17284304  accuracy:  0.93\n",
      "iteration:  11111  loss:  0.1728275  accuracy:  0.93\n",
      "iteration:  11205  loss:  0.17282379  accuracy:  0.93\n",
      "iteration:  11223  loss:  0.172743  accuracy:  0.93\n",
      "iteration:  11235  loss:  0.17262085  accuracy:  0.9233333333333333\n",
      "iteration:  11276  loss:  0.17261098  accuracy:  0.9266666666666666\n",
      "iteration:  11366  loss:  0.17256287  accuracy:  0.93\n",
      "iteration:  11382  loss:  0.17238241  accuracy:  0.9266666666666666\n",
      "iteration:  11425  loss:  0.17231217  accuracy:  0.9266666666666666\n",
      "iteration:  11465  loss:  0.17221013  accuracy:  0.93\n",
      "iteration:  11534  loss:  0.17202191  accuracy:  0.93\n",
      "iteration:  11543  loss:  0.17180066  accuracy:  0.9266666666666666\n",
      "iteration:  11573  loss:  0.1717013  accuracy:  0.93\n",
      "iteration:  11686  loss:  0.17139575  accuracy:  0.9333333333333333\n",
      "iteration:  11692  loss:  0.17126201  accuracy:  0.9333333333333333\n",
      "iteration:  11812  loss:  0.17123984  accuracy:  0.9333333333333333\n",
      "iteration:  11989  loss:  0.17121433  accuracy:  0.9266666666666666\n",
      "iteration:  12004  loss:  0.17117961  accuracy:  0.93\n",
      "iteration:  12046  loss:  0.17097297  accuracy:  0.9333333333333333\n",
      "iteration:  12095  loss:  0.17089078  accuracy:  0.9366666666666666\n",
      "iteration:  12149  loss:  0.17083694  accuracy:  0.9333333333333333\n",
      "iteration:  12263  loss:  0.17076148  accuracy:  0.9333333333333333\n",
      "iteration:  12359  loss:  0.17071079  accuracy:  0.9333333333333333\n",
      "iteration:  12868  loss:  0.17065334  accuracy:  0.9266666666666666\n",
      "iteration:  13008  loss:  0.17057313  accuracy:  0.9333333333333333\n",
      "iteration:  13174  loss:  0.17030643  accuracy:  0.9266666666666666\n",
      "iteration:  13335  loss:  0.17017329  accuracy:  0.93\n",
      "iteration:  13394  loss:  0.17013702  accuracy:  0.93\n",
      "iteration:  13407  loss:  0.17003858  accuracy:  0.9266666666666666\n",
      "iteration:  13572  loss:  0.16999125  accuracy:  0.9266666666666666\n",
      "iteration:  13641  loss:  0.16994168  accuracy:  0.9266666666666666\n",
      "iteration:  13672  loss:  0.16989873  accuracy:  0.93\n",
      "iteration:  13791  loss:  0.16985786  accuracy:  0.9333333333333333\n",
      "iteration:  14445  loss:  0.16984253  accuracy:  0.9333333333333333\n",
      "iteration:  14808  loss:  0.16980286  accuracy:  0.9366666666666666\n",
      "iteration:  15070  loss:  0.1696788  accuracy:  0.9333333333333333\n",
      "iteration:  15232  loss:  0.169658  accuracy:  0.9333333333333333\n",
      "iteration:  15379  loss:  0.1693406  accuracy:  0.9333333333333333\n",
      "iteration:  16146  loss:  0.16924408  accuracy:  0.9333333333333333\n",
      "iteration:  16668  loss:  0.16922522  accuracy:  0.9333333333333333\n",
      "iteration:  16887  loss:  0.16920641  accuracy:  0.9333333333333333\n",
      "iteration:  17072  loss:  0.16908737  accuracy:  0.9266666666666666\n",
      "iteration:  17261  loss:  0.16900454  accuracy:  0.9333333333333333\n",
      "iteration:  17501  loss:  0.16892955  accuracy:  0.93\n",
      "iteration:  17780  loss:  0.16892265  accuracy:  0.9266666666666666\n",
      "iteration:  17836  loss:  0.16890615  accuracy:  0.9266666666666666\n",
      "iteration:  18250  loss:  0.16881539  accuracy:  0.9333333333333333\n",
      "iteration:  18658  loss:  0.16868713  accuracy:  0.9333333333333333\n",
      "iteration:  18925  loss:  0.16862784  accuracy:  0.93\n",
      "iteration:  19229  loss:  0.16858304  accuracy:  0.9266666666666666\n",
      "iteration:  19367  loss:  0.1684302  accuracy:  0.9333333333333333\n",
      "iteration:  20903  loss:  0.1684177  accuracy:  0.93\n",
      "iteration:  21087  loss:  0.16828825  accuracy:  0.9333333333333333\n",
      "iteration:  21090  loss:  0.16821083  accuracy:  0.93\n",
      "iteration:  21257  loss:  0.16805127  accuracy:  0.9266666666666666\n",
      "iteration:  22167  loss:  0.16803677  accuracy:  0.9333333333333333\n",
      "iteration:  22438  loss:  0.16791964  accuracy:  0.9333333333333333\n",
      "iteration:  22552  loss:  0.16782929  accuracy:  0.9266666666666666\n",
      "iteration:  22586  loss:  0.16782898  accuracy:  0.93\n",
      "iteration:  22857  loss:  0.16768897  accuracy:  0.93\n",
      "iteration:  23187  loss:  0.16764972  accuracy:  0.9333333333333333\n",
      "iteration:  23438  loss:  0.16762747  accuracy:  0.93\n",
      "iteration:  23663  loss:  0.1675869  accuracy:  0.9333333333333333\n",
      "iteration:  24062  loss:  0.16753834  accuracy:  0.93\n",
      "iteration:  24206  loss:  0.1674528  accuracy:  0.93\n",
      "iteration:  24422  loss:  0.1674426  accuracy:  0.9333333333333333\n",
      "iteration:  25420  loss:  0.16732061  accuracy:  0.9266666666666666\n",
      "iteration:  26299  loss:  0.16721748  accuracy:  0.9266666666666666\n",
      "iteration:  26413  loss:  0.1671752  accuracy:  0.93\n",
      "iteration:  26910  loss:  0.1670629  accuracy:  0.93\n",
      "iteration:  27281  loss:  0.16704287  accuracy:  0.93\n",
      "iteration:  28185  loss:  0.16703402  accuracy:  0.9266666666666666\n",
      "iteration:  29083  loss:  0.16698252  accuracy:  0.9266666666666666\n",
      "iteration:  29604  loss:  0.16696063  accuracy:  0.9233333333333333\n",
      "iteration:  29656  loss:  0.16684018  accuracy:  0.93\n",
      "iteration:  29685  loss:  0.16674896  accuracy:  0.9266666666666666\n",
      "iteration:  30304  loss:  0.16672185  accuracy:  0.9266666666666666\n",
      "iteration:  30466  loss:  0.16658022  accuracy:  0.93\n",
      "iteration:  30595  loss:  0.16651489  accuracy:  0.9266666666666666\n",
      "iteration:  30781  loss:  0.16638951  accuracy:  0.93\n",
      "iteration:  31296  loss:  0.16628732  accuracy:  0.9266666666666666\n",
      "iteration:  31799  loss:  0.1662718  accuracy:  0.93\n",
      "iteration:  31947  loss:  0.16616748  accuracy:  0.9266666666666666\n",
      "iteration:  32064  loss:  0.1661476  accuracy:  0.9333333333333333\n",
      "iteration:  32397  loss:  0.16604705  accuracy:  0.9333333333333333\n",
      "iteration:  32509  loss:  0.16603784  accuracy:  0.93\n",
      "iteration:  32580  loss:  0.16602302  accuracy:  0.9333333333333333\n",
      "iteration:  32965  loss:  0.1659464  accuracy:  0.9233333333333333\n",
      "iteration:  33214  loss:  0.16589385  accuracy:  0.93\n",
      "iteration:  33303  loss:  0.16581355  accuracy:  0.93\n",
      "iteration:  33878  loss:  0.1657353  accuracy:  0.9233333333333333\n",
      "iteration:  33885  loss:  0.16572782  accuracy:  0.93\n",
      "iteration:  34960  loss:  0.16572364  accuracy:  0.9266666666666666\n",
      "iteration:  35347  loss:  0.16564265  accuracy:  0.9266666666666666\n",
      "iteration:  35394  loss:  0.16555491  accuracy:  0.9266666666666666\n",
      "iteration:  35783  loss:  0.1655343  accuracy:  0.9266666666666666\n",
      "iteration:  37956  loss:  0.16550794  accuracy:  0.93\n",
      "iteration:  38317  loss:  0.1654901  accuracy:  0.9266666666666666\n",
      "iteration:  38372  loss:  0.16530803  accuracy:  0.9266666666666666\n",
      "iteration:  41167  loss:  0.16523296  accuracy:  0.9266666666666666\n",
      "iteration:  42133  loss:  0.1651447  accuracy:  0.9266666666666666\n",
      "iteration:  43247  loss:  0.16509824  accuracy:  0.9266666666666666\n",
      "iteration:  43639  loss:  0.16503775  accuracy:  0.9266666666666666\n",
      "iteration:  43785  loss:  0.16501811  accuracy:  0.9266666666666666\n",
      "iteration:  44268  loss:  0.16500945  accuracy:  0.9233333333333333\n",
      "iteration:  44941  loss:  0.1648671  accuracy:  0.9266666666666666\n",
      "iteration:  45127  loss:  0.16483799  accuracy:  0.9266666666666666\n",
      "iteration:  45220  loss:  0.16475153  accuracy:  0.9266666666666666\n",
      "iteration:  51847  loss:  0.16465902  accuracy:  0.93\n",
      "iteration:  52584  loss:  0.1646144  accuracy:  0.9266666666666666\n",
      "iteration:  52786  loss:  0.16441472  accuracy:  0.9266666666666666\n",
      "iteration:  53048  loss:  0.16438957  accuracy:  0.9266666666666666\n",
      "iteration:  54495  loss:  0.16435932  accuracy:  0.93\n",
      "iteration:  54665  loss:  0.16433261  accuracy:  0.9266666666666666\n",
      "iteration:  60056  loss:  0.16431041  accuracy:  0.9266666666666666\n",
      "iteration:  61398  loss:  0.16429746  accuracy:  0.9266666666666666\n",
      "iteration:  61726  loss:  0.16429001  accuracy:  0.9233333333333333\n",
      "iteration:  61930  loss:  0.16412921  accuracy:  0.93\n",
      "iteration:  62375  loss:  0.16411497  accuracy:  0.9333333333333333\n",
      "iteration:  62714  loss:  0.16406609  accuracy:  0.9266666666666666\n",
      "iteration:  62952  loss:  0.1640659  accuracy:  0.93\n",
      "iteration:  62984  loss:  0.1635097  accuracy:  0.93\n",
      "iteration:  63308  loss:  0.16344553  accuracy:  0.9266666666666666\n",
      "iteration:  87161  loss:  0.16343929  accuracy:  0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "for iterations in range(100000):\n",
    "\n",
    "    dense1.weights+=0.05 * np.random.randn(2,3)\n",
    "    dense1.bias+=0.05 * np.random.randn(1,3)\n",
    "    dense2.weights+=0.05 * np.random.randn(3,3)\n",
    "    dense2.bias+=0.05 * np.random.randn(1,3)\n",
    "\n",
    "    dense1.forward(x)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    loss=loss_function.calculate(activation2.output,y)\n",
    "\n",
    "    predictions=np.argmax(activation2.output,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if loss<lowest_loss:\n",
    "        print(\"iteration: \",iterations, \" loss: \",loss, \" accuracy: \",accuracy)\n",
    "        best_dense1_weights=dense1.weights.copy()\n",
    "        best_dense1_bias=dense1.bias.copy()\n",
    "        best_dense2_weights=dense2.weights.copy()\n",
    "        best_dense2_bias=dense2.bias.copy()\n",
    "        lowest_loss=loss\n",
    "\n",
    "    else:\n",
    "        dense1.weights=best_dense1_weights.copy()\n",
    "        dense1.bias=best_dense1_bias.copy()\n",
    "        dense2.weights=best_dense2_weights.copy()\n",
    "        dense2.bias=best_dense2_bias.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
