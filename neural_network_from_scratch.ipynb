{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network implementation using only NumPy.\n",
    "\n",
    "### Problem Statement: \n",
    "* In this project, we will learn the implementation of Neural Network from scratch using numpy.\n",
    "\n",
    "### General Architecture:\n",
    "- __Input Layer__:\n",
    "    The input layer will have 2400 data points. \n",
    "- __Hidden Layer__:\n",
    "    Typically, we start with a few hidden layer with ReLU activation function.\n",
    "- __Output Layer__:\n",
    "    This layer will have 3 units with Softmax activation function to output class probabilities.\n",
    "\n",
    "### Objective:\n",
    "* Create a dataset using `nnfs` library.\n",
    "* Implement neural network architecture on the created dataset.\n",
    "* Train the neural network using forward propagation.\n",
    "* Evaluate the accuracy of the neural network. \n",
    "\n",
    "### Dataset:\n",
    "* The dataset is created using `vertical_data` method from nnfs library.\n",
    "* `vertical_data()` function generates or fetches a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a fully connected dense layer of neural network using `Layer_Dense` class.**\n",
    "\n",
    "### Initialization:\n",
    "\n",
    "* `n_inputs`: The number of input features to the layer.\n",
    "* `n_neurons`: The number of neurons in the layer.\n",
    "* `weights`: The weight matrix initialized with small random values. It has a shape of (n_inputs, n_neurons).\n",
    "* `bias`: The bias vector initialized to zeros. It has a shape of (1, n_neurons).\n",
    "\n",
    "### Forward Pass:\n",
    "\n",
    "* `input`: The input to the layer, which is typically the output of the previous layer or the input data.\n",
    "* `output`: The output of the layer, computed as the dot product of the input and the weights, plus the bias.\n",
    "* The forward pass is essential for propagating the input through the network to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights= 0.10 * np.random.randn(n_inputs,n_neurons)  \n",
    "        self.bias= np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = np.dot(input, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement ReLU activation function.**\n",
    "\n",
    "* ReLU enables neural networks to learn and generalize complex patterns effectively.\n",
    "\n",
    "* ReLU is defined as:\n",
    "\n",
    "$$ReLU(ùë•)=max(0,ùë•)$$\n",
    "\n",
    "\n",
    "* This means that the function outputs the given input itself, if it is positive; otherwise, the output is zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLu:\n",
    "    def forward(self,input):\n",
    "        self.output=np.maximum(0,input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement activation softmax function.**\n",
    "\n",
    "* The softmax function is commonly used in the output layer of a neural network for classification tasks. \n",
    "* It converts raw output scores into probabilities, making it easier to interpret the output of the model.\n",
    "\n",
    "\\[\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}\n",
    "\\]\n",
    "\n",
    "This formula ensures that the output is a probability distribution, i.e., the sum of all the output probabilities is 1, and each probability is between 0 and 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self,input):\n",
    "        exp_value=np.exp(input-np.max(input, axis=1,keepdims=True))\n",
    "        probability=exp_value/np.sum(exp_value,axis=1,keepdims=True)\n",
    "        self.output=probability\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss calculation**\n",
    "\n",
    "To quantify the difference between the predicted output of a neural network and the actual ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses=self.forward(output,y)\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing categorial cross-entropy loss**\n",
    "It measures the dissimilarity between the predicted probability distribution (output of the model) and the true probability distribution (one-hot encoded target labels).\n",
    "\n",
    "* The forward method takes two arguments: `y_pred` (predicted probabilities) and `y_true` (true class labels).\n",
    "* It first clips the predicted probabilities to avoid numerical instability issues.\n",
    "* In next step, it calculates the correct confidence scores based on the true class labels.\n",
    "* Finally, it computes the negative log-likelihoods as the loss values and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples=len(y_pred)\n",
    "        y_pred_clipped=np.clip(y_pred, 1e-7,1-1e-7)\n",
    "\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidence=y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidence=np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        negative_log_liklihoods= -np.log(correct_confidence)\n",
    "        return negative_log_liklihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate sample dataset**\n",
    "\n",
    "To create a dataset, we will use `vertical_data` method from `nnfs.datasets`, that generates a dataset. \n",
    "\n",
    "\n",
    "The function takes two arguments:\n",
    "* `samples` : 800 - This specifies that the function should generate 800 samples in the dataset.\n",
    "* `classes` : 3 - This specifies that the dataset should have 3 distinct classes or categories for the target labels `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = vertical_data(samples=800,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Column1   Column2  Labels\n",
      "0  0.176405  0.641117       0\n",
      "1  0.040016  0.578580       0\n",
      "2  0.097874  0.494253       0\n",
      "3  0.224089  0.460878       0\n",
      "4  0.186756  0.594092       0\n",
      "\n",
      "2400 Rows & 3 Cols.\n"
     ]
    }
   ],
   "source": [
    "# Visually, this is how our dataset looks like:\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(x, columns=['Column1', 'Column2'])\n",
    "df['Labels'] = y\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(df.head())\n",
    "print(f'\\n{rows} Rows & {cols} Cols.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the architecture of neural network using layers and activation functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1=Layer_Dense(2,3)\n",
    "activation1=Activation_ReLu()\n",
    "\n",
    "dense2=Layer_Dense(3,3)\n",
    "activation2=Activation_Softmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=Loss_CategoricalCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tracking Best Model Parameters**\n",
    "\n",
    "In this part of the code, we are initializing variables to track the parameters of the best model found during training. These parameters include weights and biases of two dense layers (dense1 and dense2).\n",
    "<br>\n",
    "<br>\n",
    "Also, we have initialized the loss with a large value (9999999) to ensure that any loss calculated during training will be lower than this initial value. \n",
    "<br>\n",
    "<br>\n",
    "This variable will be updated during training if a model with a lower loss is found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_loss=9999999\n",
    "best_dense1_weights=dense1.weights.copy()\n",
    "best_dense1_bias=dense1.bias.copy()\n",
    "best_dense2_weights=dense2.weights.copy()\n",
    "best_dense2_bias=dense2.bias.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  loss:  1.1013985 accuracy: 0.33\n",
      "iteration:  50  loss:  1.0087012 accuracy: 0.62\n",
      "iteration:  100  loss:  0.89863473 accuracy: 0.78\n",
      "iteration:  150  loss:  0.7762941 accuracy: 0.67\n",
      "iteration:  200  loss:  0.7016407 accuracy: 0.86\n",
      "iteration:  250  loss:  0.5616108 accuracy: 0.78\n",
      "iteration:  300  loss:  0.4245901 accuracy: 0.92\n",
      "iteration:  350  loss:  0.40391943 accuracy: 0.89\n",
      "iteration:  400  loss:  0.33655158 accuracy: 0.92\n",
      "iteration:  450  loss:  0.2871132 accuracy: 0.93\n",
      "iteration:  500  loss:  0.25195062 accuracy: 0.94\n",
      "iteration:  550  loss:  0.22821032 accuracy: 0.94\n",
      "iteration:  600  loss:  0.20495217 accuracy: 0.93\n",
      "iteration:  650  loss:  0.18344648 accuracy: 0.93\n",
      "iteration:  700  loss:  0.17568047 accuracy: 0.94\n",
      "iteration:  750  loss:  0.18093361 accuracy: 0.93\n",
      "iteration:  800  loss:  0.18032336 accuracy: 0.94\n",
      "iteration:  850  loss:  0.16782731 accuracy: 0.93\n",
      "iteration:  900  loss:  0.16415131 accuracy: 0.94\n",
      "iteration:  950  loss:  0.16705398 accuracy: 0.93\n",
      "iteration:  1000  loss:  0.1559029 accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "for iterations in range(1001):\n",
    "\n",
    "    dense1.weights+=0.05 * np.random.randn(2,3)\n",
    "    dense1.bias+=0.05 * np.random.randn(1,3)\n",
    "    dense2.weights+=0.05 * np.random.randn(3,3)\n",
    "    dense2.bias+=0.05 * np.random.randn(1,3)\n",
    "\n",
    "    dense1.forward(x)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    loss=loss_function.calculate(activation2.output,y)\n",
    "\n",
    "    predictions=np.argmax(activation2.output,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if (iterations%50==0):\n",
    "        print(\"iteration: \",iterations, \" loss: \",loss, f\"accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "    if loss<lowest_loss:\n",
    "        best_dense1_weights=dense1.weights.copy()\n",
    "        best_dense1_bias=dense1.bias.copy()\n",
    "        best_dense2_weights=dense2.weights.copy()\n",
    "        best_dense2_bias=dense2.bias.copy()\n",
    "        lowest_loss=loss\n",
    "\n",
    "    else:\n",
    "        dense1.weights=best_dense1_weights.copy()\n",
    "        dense1.bias=best_dense1_bias.copy()\n",
    "        dense2.weights=best_dense2_weights.copy()\n",
    "        dense2.bias=best_dense2_bias.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~94% Accuracy achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
